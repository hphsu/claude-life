# LLM Integration for Fortune Telling System

## Overview

The fortune-telling system now supports three LLM providers with automatic fallback:

1. **CLAUDE_CODE** (via slash command) - No API key needed
2. **OPENAI** (GPT-4) - Requires OPENAI_API_KEY
3. **ANTHROPIC** (Claude) - Requires ANTHROPIC_API_KEY
4. **NONE** - Traditional rule-based analysis only

## Provider Priority

The system auto-detects providers in this order:

1. **Claude Code** (if `claude --version` succeeds)
2. **OpenAI** (if OPENAI_API_KEY environment variable exists)
3. **Anthropic** (if ANTHROPIC_API_KEY environment variable exists)
4. **None** (fallback to traditional analysis)

## Usage

### Automatic Mode (Python Script)

```python
from fortune_telling.llm_analyzer import get_llm_analyzer

# Auto-detect best available provider
analyzer = get_llm_analyzer()

# Force specific provider
from fortune_telling.llm_analyzer import LLMProvider
analyzer = get_llm_analyzer(provider=LLMProvider.OPENAI)
```

When running `run_frank_analysis.py`:
- If no API keys are set, uses traditional analysis
- If OPENAI_API_KEY or ANTHROPIC_API_KEY is set, uses that provider
- Falls back to traditional analysis if LLM fails

### Slash Command Mode (Claude Code)

The CLAUDE_CODE provider is designed for **interactive use** via slash command:

```bash
/fortune-tell
```

This command embeds the system prompts and allows Claude Code to provide analysis directly.

**Note**: CLAUDE_CODE cannot be used when running Python scripts directly because:
- Task tool cannot be called recursively from within Python execution
- Subprocess `claude --print` calls timeout (120 seconds)
- Better to use OPENAI or ANTHROPIC providers for automated runs

## Architecture

### Fallback Mechanism

All analysis functions use `analyze_with_fallback()` pattern:

```python
llm_analyzer = get_llm_analyzer()

result = llm_analyzer.analyze_with_fallback(
    system_prompt=BAZI_SYSTEM_PROMPT,
    analysis_prompt=construct_bazi_personality_prompt(bazi_data),
    fallback_func=_traditional_personality_analysis,
    fallback_args=(bazi_data,),
    min_length=300,
    temperature=0.7,
    max_tokens=4000
)
```

**Return Types:**
- **String**: LLM analysis succeeded (â‰¥300 characters)
- **Dict**: Fallback was used (traditional analysis)

The calling code must handle both return types:

```python
if isinstance(llm_result, str) and len(llm_result.replace(' ', '').replace('\n', '')) >= 300:
    # LLM success - enhance traditional result
    traditional_result = _traditional_personality_analysis(bazi_data)
    traditional_result['llm_analysis'] = llm_result
    traditional_result['confidence_level'] = confidence
    traditional_result['analysis_method'] = 'LLM enhanced'
    return traditional_result
elif isinstance(llm_result, dict):
    # Fallback was already executed, return it directly
    return llm_result
```

### Quality Standards

LLM output must meet minimum character requirements:
- **Personality/Career/Wealth**: â‰¥300 characters
- **Palace Analysis**: â‰¥250 characters
- **Synthesis**: â‰¥400 characters

If output is too short, system falls back to traditional analysis.

## File Structure

```
scripts/fortune_telling/
â”œâ”€â”€ llm_analyzer.py                  # Core LLM integration layer
â”œâ”€â”€ prompt_utils.py                  # System prompt loading utilities
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ bazi_system_prompt.md       # å…«å­— analysis framework
â”‚   â”œâ”€â”€ ziwei_system_prompt.md      # ç´«å¾® analysis framework
â”‚   â”œâ”€â”€ astrology_system_prompt.md  # å æ˜Ÿ analysis framework
â”‚   â””â”€â”€ synthesis_system_prompt.md  # Cross-method synthesis framework
â”œâ”€â”€ bazi_interpretation.py          # å…«å­— interpretation with LLM
â”œâ”€â”€ ziwei_interpretation.py         # ç´«å¾® interpretation with LLM
â”œâ”€â”€ astrology_interpretation.py     # å æ˜Ÿ interpretation with LLM
â””â”€â”€ synthesis_engine.py             # Cross-method synthesis with LLM

.claude/commands/
â””â”€â”€ fortune-tell.md                  # Slash command for Claude Code integration
```

## Configuration

### Environment Variables

```bash
# Option 1: Use OpenAI
export OPENAI_API_KEY="sk-..."

# Option 2: Use Anthropic
export ANTHROPIC_API_KEY="sk-ant-..."

# Option 3: No API key (uses traditional analysis)
# Just don't set any keys
```

### Manual Provider Selection

```python
from fortune_telling.llm_analyzer import LLMProvider, LLMAnalyzer

# Force OpenAI
analyzer = LLMAnalyzer(provider=LLMProvider.OPENAI, api_key="sk-...")

# Force Anthropic
analyzer = LLMAnalyzer(provider=LLMProvider.ANTHROPIC, api_key="sk-ant-...")

# Force traditional only
analyzer = LLMAnalyzer(provider=LLMProvider.NONE)
```

## HTML Report Integration

When LLM analysis is available, the HTML report includes:

- **ğŸ¤– AIæ·±åº¦åˆ†æ sections**: Blue-gradient sections with LLM output
- **Confidence badges**: Color-coded based on data quality
- **Method comparison**: Shows convergent traits (3-method consensus)

Example:
```html
<div class="llm-analysis-section">
    <span class="llm-badge">ğŸ¤– AIæ·±åº¦åˆ†æ</span>
    <div class="llm-content">
        <!-- LLM analysis text -->
    </div>
</div>
```

## Troubleshooting

### Claude Code Provider Warnings

If you see:
```
CLAUDE_CODE provider ç„¡æ³•åœ¨é‹è¡Œæ™‚è‡ªå‹•èª¿ç”¨ã€‚
è«‹æ”¹ç”¨ OPENAI æˆ– ANTHROPIC provider
```

**Solution**: This is expected behavior when running Python scripts. Either:
1. Set OPENAI_API_KEY or ANTHROPIC_API_KEY environment variable
2. Use `/fortune-tell` slash command for interactive analysis
3. Accept traditional analysis (system will fallback automatically)

### LLM Timeout

If LLM calls timeout or fail:
- System automatically falls back to traditional analysis
- No interruption to workflow
- Traditional analysis is still comprehensive

### Short LLM Output

If LLM returns <300 characters:
```
âš ï¸ LLMè¼¸å‡º240å­—ï¼Œæœªé”300å­—æ¨™æº–ï¼Œä½¿ç”¨fallback
```

**Solution**:
- System automatically uses traditional analysis
- Try different temperature (0.7-0.9)
- Try different model (gpt-4-turbo vs gpt-4)

## Examples

### Full Analysis with LLM

```bash
# Set API key
export OPENAI_API_KEY="sk-..."

# Run analysis
python run_frank_analysis.py

# Output:
# âœ… å…«å­—æ·±åº¦è§£é‡‹å®Œæˆ  (with LLM)
# âœ… ç´«å¾®æ–—æ•¸æ·±åº¦è§£é‡‹å®Œæˆ  (with LLM)
# âœ… å¿ƒç†å æ˜Ÿæ·±åº¦è§£é‡‹å®Œæˆ  (with LLM)
# âœ… ç¶œåˆåˆ†æå®Œæˆ  (with LLM)
```

### Traditional-Only Analysis

```bash
# No API keys set
python run_frank_analysis.py

# Output:
# â„¹ï¸ LLMä¸å¯ç”¨ï¼Œä½¿ç”¨å‚³çµ±åˆ†ææ–¹æ³•
# âœ… å…«å­—æ·±åº¦è§£é‡‹å®Œæˆ  (traditional)
# âœ… ç´«å¾®æ–—æ•¸æ·±åº¦è§£é‡‹å®Œæˆ  (traditional)
# âœ… å¿ƒç†å æ˜Ÿæ·±åº¦è§£é‡‹å®Œæˆ  (traditional)
# âœ… ç¶œåˆåˆ†æå®Œæˆ  (traditional)
```

### Interactive Claude Code Analysis

```bash
# In Claude Code terminal
/fortune-tell

# Then provide system prompt + analysis data
# Claude Code will analyze directly
```

## Performance Considerations

- **LLM Analysis**: 2-10 seconds per domain (depends on provider)
- **Traditional Analysis**: <1 second per domain
- **Total Runtime**: 10-60 seconds for full analysis (with LLM)
- **Fallback Impact**: Adds 1-2 seconds for failed LLM attempts

## Future Enhancements

1. **Caching**: Cache LLM results for same birth data
2. **Batch Processing**: Analyze multiple domains in parallel
3. **Fine-tuning**: Train custom models on fortune-telling corpus
4. **Quality Scoring**: Automatically score LLM output quality
5. **Hybrid Analysis**: Combine LLM insights with traditional rules
